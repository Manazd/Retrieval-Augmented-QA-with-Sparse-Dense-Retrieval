{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0e4f49-0952-4e29-8abf-eef049810385",
   "metadata": {},
   "source": [
    "# COMS W4705 - Homework 4\n",
    "## Question Answering with Retrieval Augmented Generation\n",
    "\n",
    "Anubhav Jangra \\<aj3228@columbia.edu\\>, Emile Al-Billeh \\<ea3048@columbia.edu\\>, Daniel Bauer \\<bauer@cs.columbia.edu\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adabf07-f8d4-4e8c-82d5-1a89d3656dd4",
   "metadata": {},
   "source": [
    "In this assignment, you will use a pretrained LLM for question answering on a subset of the Stanford QA Dataset (SQuAD). Here is an example question from SQuAD: \n",
    "\n",
    "> *Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?*\n",
    "\n",
    "Specific domain knowledge to answer questions like this may not be available in the data that the LLM was pre-trained on. As a result, if we simply prompt the the LLM to answer this question, it may tell us that it does not know the answer, or worse, it may hallucinate an incorrect answer. Even if we are lucky and the LLM has have enough information to answer this question from pre-training, but the information may be outdated (the headmaster is likely to change from time to time). \n",
    "\n",
    "Luckily, SQuAD provides a context snippet for each question that may contain the answer, such as\n",
    "\n",
    "> *The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. **The school's headmaster, history professor Juan Pedro Toni**, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.*\n",
    "\n",
    "If we include the context as part of the prompt to the LLM, the model should be able to correctly answer the question (SQuAD contains \"unanswerable questions\", for which the provided context does not provide sufficient information to answer the question -- we will ignore these for the purpose of this assignment).\n",
    "\n",
    "We will consider a scenario in which we don't know which context belongs to which question and we will use **Retrieval Augmented Generation (RAG)** techniques to identify the relevant context from the set of all available contexts. \n",
    "\n",
    "Specifically we will experiment with the following systems: \n",
    "\n",
    "* A baseline \"vanilla QA\" system in which we try to answer the question without any additional context (i.e. using the pre-trained LLM only).\n",
    "* An \"oracle\" system, in which we provide the correct context for each question. This establishes an upper bound for the retrieval approaches. \n",
    "* Two different approaches for retrieving relevant contexts:\n",
    "  * based on token overlap between the question and each context.\n",
    "  * based on cosine similarity between question embeddings and candidate context embeddings (obtained using BERT).\n",
    "    \n",
    "We will evaluate each system using a number of metrics commonly used for QA tasks: \n",
    "* Exact Match (EM), which measures the percentage of predictions that exactly match the ground truth answers.\n",
    "* F1 score, measured on the token overlap between the predicted and ground truth answers.\n",
    "* ROUGE (specifically, ROUGE2)\n",
    "\n",
    "Follow the instructions in this notebook step-by step. Much of the code is provided and just needs to be run, but some sections are marked with todo. Make sure to complete all these sections.\n",
    "\n",
    "\n",
    "Requirements: \n",
    "Access to a GPU is required for this assignment. If you have a recent mac, you can try using mps. Otherwise, I recommend renting a GPU instance through a service like vast.ai or lambdalabs. Google Colab can work in a pinch, but you would have to deal with quotas and it's somewhat easy to lose unsaved work.\n",
    "\n",
    "First, we need to ensure that transformers is installed, as well as the accelerate package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a69e38-7395-4361-a3ff-16ed52a89e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (3.20.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp314-cp314-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[?25l\u001b[33m  WARNING: The script tqdm is installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [huggingface-hub]\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m6/7\u001b[0m [transformers]\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac570bdf-90b7-4dbc-b0df-2377d108f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.12.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115172-d4cd-448e-a3a2-63d22fe7cbef",
   "metadata": {},
   "source": [
    "Now all the relevant imports should succeed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8276df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee393771",
   "metadata": {},
   "source": [
    "Set GPU for Mac:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d6b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ec0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63708594",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This section creates the benchmark data we need to evaluate the QA systems. It has already been implemented for you. We recommend that you run it only once, save the benchmark data in a json file and then load it when needed. The following code may not work in Windows. We are providing the pre-generated benchmark data for download as an alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22da799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./squad_data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e31f8-1b85-47bb-9bd3-60dfa76ccea4",
   "metadata": {},
   "source": [
    "### Downloading the Data and Creating the Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b74477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 40.1M  100 40.1M    0     0  13.1M      0  0:00:03  0:00:03 --:--:-- 13.1M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "\n",
    "os.system(f\"curl -L {training_url} -o {data_dir}/squad_train.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d634aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 442\n",
      "==============================\n",
      "For topic \"Beyoncé\"\n",
      "Number of available context paragraphs: 66\n",
      "==============================\n",
      "The first paragraph is:\n",
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "==============================\n",
      "The first five question-answer pairs are:\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "--------------------\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "--------------------\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "--------------------\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "--------------------\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# load the raw dataset\n",
    "train_data = json.load(open(f\"{data_dir}/squad_train.json\"))\n",
    "\n",
    "# Some details about the dataset\n",
    "\n",
    "# SQuAD is split up into questions about a number of different topics\n",
    "print(f\"Number of topics: {len(train_data['data'])}\")\n",
    "\n",
    "# Let's explore just one topic. Each topic comes with a number of context paragraphs. \n",
    "print(\"=\"*30)\n",
    "print(f\"For topic \\\"{train_data['data'][0]['title']}\\\"\")\n",
    "print(f\"Number of available context paragraphs: {len(train_data['data'][0]['paragraphs'])}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"The first paragraph is:\")\n",
    "print(train_data['data'][0]['paragraphs'][0]['context'])\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Each paragraph comes with a number of question/answer pairs about the text in the paragraph\n",
    "print(\"The first five question-answer pairs are:\")\n",
    "for qa in train_data['data'][0]['paragraphs'][0]['qas'][:5]:\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answers'][0]['text']}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17faec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paragraphs in the training set: 19035\n",
      "Total number of question-answer pairs in the training set: 130319\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of paragraphs in the training set:\", sum([len(topic['paragraphs']) for topic in train_data['data']]))\n",
    "print(\"Total number of question-answer pairs in the training set:\", sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf17aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg number of answers per question: 0.6662190471074824\n",
      "Count of answerable vs unanswerable questions:\n",
      "Answerable questions: 86821 (66.62%)\n",
      "Unanswerable questions: 43498 (33.38%)\n"
     ]
    }
   ],
   "source": [
    "# not all questions are answerable given the information in the paragraph. Part of the original SQuaD 2 task is to identify such\n",
    "# unanswerable questions. We will ignore them for the purpose of this assignment. \n",
    "print(\"Avg number of answers per question:\", \n",
    "      sum([len(qa['answers']) for topic in train_data['data'] for paragraph in topic['paragraphs'] for qa in paragraph['qas']]) / \n",
    "      sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))\n",
    "print(\"Count of answerable vs unanswerable questions:\")\n",
    "answerable_count = 0\n",
    "unanswerable_count = 0\n",
    "for topic in train_data['data']:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0:\n",
    "                answerable_count += 1\n",
    "            else:\n",
    "                unanswerable_count += 1\n",
    "print(f\"Answerable questions: {answerable_count} ({answerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")\n",
    "print(f\"Unanswerable questions: {unanswerable_count} ({unanswerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b13dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, create the RAG QA benchmark consisting of 250 answerable questions. \n",
    "\n",
    "# We will use all available context paragraphs for RAG\n",
    "rag_contexts = [paragraph['context'] for topic in train_data['data'] for paragraph in topic['paragraphs']]\n",
    "\n",
    "qa_pairs = []\n",
    "for topic in train_data['data']:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            if len(qa['answers']) > 0:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": qa['question'],\n",
    "                    \"answer\": qa['answers'][0]['text'],\n",
    "                    \"context\": paragraph['context']\n",
    "                })\n",
    "            \n",
    "# randomly sample 250 answerable questions for the benchmark\n",
    "import random\n",
    "random.seed(42) # IMPORTANT so everyone is working on the same set of sampled QA pairs\n",
    "sampled_qa_pairs = random.sample(qa_pairs, 250)\n",
    "\n",
    "\n",
    "evaluation_benchmark = {'qas': sampled_qa_pairs, \n",
    "                        'contexts': rag_contexts}\n",
    "random.shuffle(evaluation_benchmark['qas'])\n",
    "random.shuffle(evaluation_benchmark['contexts'])\n",
    "\n",
    "# save the evaluation benchmark to a file\n",
    "json.dump(evaluation_benchmark, open(f\"{data_dir}/rag_qa_benchmark.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c447a74",
   "metadata": {},
   "source": [
    "### Loading the Benchmark Dataset / Understanding the Data Format\n",
    "\n",
    "Use the following code to load the benchmark data from a file. Take a look at the example output to see how the data is structured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d8507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample RAG contexts:\n",
      "Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\n",
      "--------------------\n",
      "Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family's ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father's tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).\n",
      "--------------------\n",
      "==============================\n",
      "Sample RAG QA pairs:\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Answer: professor Juan Pedro Toni\n",
      "--------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Answer: about six to four\n",
      "--------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Answer: 1890s\n",
      "--------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Answer: 1914\n",
      "--------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Answer: artillery\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# load the benchmark and display some samples\n",
    "evaluation_benchmark = json.load(open(f\"{data_dir}/rag_qa_benchmark.json\"))\n",
    "\n",
    "print(\"Sample RAG contexts:\")\n",
    "for context in evaluation_benchmark['contexts'][:2]:\n",
    "    print(context)\n",
    "    print(\"-\"*20)\n",
    "print(\"=\"*30)\n",
    "print(\"Sample RAG QA pairs:\")\n",
    "for qa in evaluation_benchmark['qas'][:5]:\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answer']}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa58b5-2b8a-41c7-b2f5-efaeb6d2542e",
   "metadata": {},
   "source": [
    "The `evaluation_benchmark` is a dictionary with two keys: \n",
    "* `evaluation_benchmark['qas']`  provides a list of *qa_items* (see below).\n",
    "* `evaluation_benchmark['contexts']` provides a list of available candidate contexts. Note that this includes all contexts from SQuAD, not just the ones for the 250 questions we sampled for the benchmark.\n",
    "\n",
    "Each *qa_item* is a dictionary with the following keys: \n",
    "* `qa_item['question']` is the question string\n",
    "* `qa_item['answer']` is the target answer string\n",
    "* `qa_item['context']` is the gold context for this question\n",
    "\n",
    "For example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32bce248-2f3f-47a9-8940-aecec5f9f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_items = evaluation_benchmark['qas']\n",
    "len(qa_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d428ec2b-98d7-4760-bad7-9e7ef3d01d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item = qa_items[0]\n",
    "qa_item['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ede20e1-82be-46f4-9fc6-7da84835f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'professor Juan Pedro Toni'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94856286-ac1f-4256-a279-b013202e84c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6cd6b",
   "metadata": {},
   "source": [
    "## Part 1 - Question Answering Evaluation Functions\n",
    "\n",
    "In this section. we will define a number of evaluation functions that measure the quality of the QA output, compared to a single target answer for each question. \n",
    "\n",
    "Because the evaluation will happen at a token leve, we will perform some simple pre-processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fc9fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fe47f-bbd6-4f48-b641-56d279c05ab8",
   "metadata": {},
   "source": [
    "First, Exact Match (EM) measures the percentage of predictions that match any one of the ground truth answers exactly after normalization.\n",
    "The following function returns 1 if the predicted answer is correct and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37139082-6480-47b1-b712-b2c228f08c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41af10d-f971-46ff-b917-709644d22f8a",
   "metadata": {},
   "source": [
    "The next function calculates the $F_1$ score of the set of predicted tokens against the set of target tokens. \n",
    "$F_1$ is the harmonic mean of precision and recall, providing a balance between the two. Specifically \n",
    "\n",
    "$F_1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
    "\n",
    "where $\\text{precision}$ is the fraction of predicted tokens that also appear in the target and $\\text{recall}$ is the fraction of target tokens that also appear in the prediction. \n",
    "\n",
    "**TODO**: Write the function compute_f1(a_gold, a_pred) that returns the F1 score as defined above. It should work similar to the compute_exact method above. Test your function on a sample answer and prediction to verify that it works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5c58004-6eb5-434c-9436-ae9adaf9799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred): # Complete the function\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "\n",
    "  if len(gold_toks) == 0 and len(pred_toks) == 0:\n",
    "    return 1.0\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    return 0.0\n",
    "\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "\n",
    "  if num_same == 0:\n",
    "    return 0.0\n",
    "\n",
    "  precision = num_same / len(pred_toks)\n",
    "  recall = num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97bea5d9-6e0e-4c49-a277-c1f9259078a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8571428571428571\n",
      "F1 exact: 1.0\n",
      "F1 no overlap: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "a_gold = \"professor Juan Pedro Toni\"\n",
    "a_pred = \"Juan Pedro Toni\"\n",
    "print(\"F1:\", compute_f1(a_gold, a_pred))\n",
    "\n",
    "print(\"F1 exact:\", compute_f1(\"the cat\", \"cat\"))\n",
    "print(\"F1 no overlap:\", compute_f1(\"red car\", \"blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749c7ec-0bce-420b-a614-9374ebbb0ef8",
   "metadata": {},
   "source": [
    "Finally, we are also want to compute ROUGE-2 scores (which extends the F1 score above to 2-grams). We can use the `rouge_score` package to do this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5c82b5e-f256-4bdf-a4b5-9eac65f7e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from rouge_score) (2.3.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from rouge_score) (1.17.0)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nltk->rouge_score) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=0bd3b949fa641b525f726e5c9deb018de458725296f0db8b3431cccd4782f88f\n",
      "  Stored in directory: /Users/manazadeh/Library/Caches/pip/wheels/fa/77/62/e75e60437a9821bbc6523332a7453f56b0720535477de63f75\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: joblib, click, absl-py, nltk, rouge_score\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [nltk]\u001b[33m  WARNING: The script nltk is installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [rouge_score]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 click-8.3.1 joblib-1.5.3 nltk-3.9.2 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ef6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=False)\n",
    "\n",
    "def compute_rouge2(a_gold, a_pred):\n",
    "    if not a_gold or not a_pred:\n",
    "        return 0.0\n",
    "    scores = rouge_scorer.score(a_gold.lower(), a_pred.lower())\n",
    "    return scores['rouge2'].fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e68d8-9760-486b-a07b-78c24efd636f",
   "metadata": {},
   "source": [
    "Let's test the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f2cd8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Answers:\n",
      "Original:\n",
      "Reference: London | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: london | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.4\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Original:\n",
      "Reference: The capital of England is London. | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: capital of england is london | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.888888888888889\n",
      "ROUGE-2 F1-score: 0.5714285714285715\n",
      "----------------------------------------\n",
      "Original:\n",
      "Reference: London is the capital city of England. | Predicted: London, capital of England\n",
      "Normalized:\n",
      "Reference: london is capital city of england | Predicted: london capital of england\n",
      "Exact Match: 0\n",
      "F1 Score: 0.8\n",
      "ROUGE-2 F1-score: 0.25\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reference_answers = [\"London\", \"The capital of England is London.\", \"London is the capital city of England.\"]\n",
    "predicted_answers = [\"London, capital of England\"] * len(reference_answers)\n",
    "\n",
    "print(\"Normalized Answers:\")\n",
    "for ref, pred in zip(reference_answers, predicted_answers):\n",
    "    print(f\"Original:\")\n",
    "    print(f\"Reference: {ref} | Predicted: {pred}\")\n",
    "    print(f\"Normalized:\")\n",
    "    print(f\"Reference: {normalize_answer(ref)} | Predicted: {normalize_answer(pred)}\")\n",
    "    print(\"Exact Match:\", compute_exact(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"F1 Score:\", compute_f1(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"ROUGE-2 F1-score:\", compute_rouge2(normalize_answer(ref), normalize_answer(pred)))\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0f4bc",
   "metadata": {},
   "source": [
    "## Part 2 - Vanilla Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f620b-6326-498e-80cf-c4886c1b3edd",
   "metadata": {},
   "source": [
    "In this part, we will use an off-the-shelf pretrained LLM and attempt to answer the questions from its pretraining knowledge only. \n",
    "To make things simple, we will use the huggingface transformer pipeline abstraction. The pipeline will download the model and parameters for us on creation. When we pass an input prompt to the pipeline, it will automatically perform preprocessing (tokenization), inference, and postprocessing (removing EOS markers and padding).\n",
    "\n",
    "### Loading the LLM\n",
    "The LLM we will use is the 1B version of the instruction tuned OLMo2 model. OLMo is an open source language model created by Allen AI and the University of Washington. Unlike other open source models, OLMo is also open data. You can read more about it here: https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct and here https://allenai.org/olmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf584e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "qa_model = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check which GPU device to use. Note, this will likely NOT work on a CPU. \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qa_model,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "# I just wanted to make sure the model is on the correct device\n",
    "print(pipe.model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b1078-2f98-4ca3-b070-f29854ecfd23",
   "metadata": {},
   "source": [
    "We can now pass a prompt to the model and retreive the completed answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75e256c3-e801-43fc-af45-ead3d31da487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"My favorite thing to do in fall is to make a wreath. Here are the supplies you need:\\n\\n    - A wreath mold (you can find one at most craft stores)\\n    - Yarn (green, orange, and red)\\n    - Hot glue gun and glue sticks\\n    - Cork or foam balls (for hanging)\\n    - Decorative elements (like leaves, acorns, or dried flowers)\\n\\nHere's how to make a wreath:\\n\\n1. Start by cutting the yarn to the desired length for your wreath.\\n2. Wrap the yarn around the wreath mold, leaving some extra length on each side for hanging. Secure each end of the yarn\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My favorite thing to do in fall is\"\n",
    "output = pipe(prompt, \n",
    "              max_new_tokens=128,\n",
    "              do_sample=True, # set to False for greedy decoding below\n",
    "              pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e790a89-bdad-4bd7-890d-79cc564c7f76",
   "metadata": {},
   "source": [
    "We can skip the prompt that is repeated in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73e6c0c3-d45a-4cbd-977d-269274c160ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to make a wreath. Here are the supplies you need:\\n\\n    - A wreath mold (you can find one at most craft stores)\\n    - Yarn (green, orange, and red)\\n    - Hot glue gun and glue sticks\\n    - Cork or foam balls (for hanging)\\n    - Decorative elements (like leaves, acorns, or dried flowers)\\n\\nHere's how to make a wreath:\\n\\n1. Start by cutting the yarn to the desired length for your wreath.\\n2. Wrap the yarn around the wreath mold, leaving some extra length on each side for hanging. Secure each end of the yarn\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c1915-5ee3-4a78-ad82-557154486eae",
   "metadata": {},
   "source": [
    "### Using the LLM for Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c365-c1ef-4bec-a991-e388e375e61b",
   "metadata": {},
   "source": [
    "**TODO:** Write a function `vanilla_qa(qa_item)` that take a qa_item in the format described above, inserts the question (and only the question!) into a suitable prompt, passes the prompt to the LLM and then returns the answer as a string. \n",
    "\n",
    "A prompt might look like this, but will need a bit of prompt engineering to make it work well. \n",
    "\n",
    "> *Answer the following question concisely.* \n",
    ">\n",
    "> *Question: Who played he lead role in Alien?*\n",
    "> \n",
    "> *Answer:*\n",
    "\n",
    "Once you have a basic version of the vanilla QA system you can tune the prompt (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26b94d1b-a91a-495e-9589-f241505c429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_qa(qa_item): # Complete this function\n",
    "    question = qa_item[\"question\"]\n",
    "\n",
    "    prompt = (\n",
    "        # Before experimenting with different prompt formats:\n",
    "        #\"Answer the following question concisely.\\n\\n\"\n",
    "        #f\"Question: {question}\\n\"\n",
    "        #\"Answer:\"\n",
    "\n",
    "        # After experimenting with different prompt formats:\n",
    "        \"Answer the question using only a short phrase.\\n\\n\"\n",
    "        \"Example:\\n\"\n",
    "        \"Question: Who wrote Pride and Prejudice?\\n\"\n",
    "        \"Answer: Jane Austen\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,  # set to false because of greedy decoding\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated = output[0][\"generated_text\"]\n",
    "    answer = generated[len(prompt):].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a406ff-4a0f-4641-a34c-eb5559779dd4",
   "metadata": {},
   "source": [
    "The following code should return an answer (but possibly not the right one) to the first question in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e4df6c1-8533-4744-9f52-2aa48c088eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_item = evaluation_benchmark['qas'][0]\n",
    "qa_item['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9e4cfb5-4eb6-41fb-9aef-aac01451e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The headmaster of the Christian Brothers of Ireland Stella Maris College is Fr Michael O'Connell.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_qa(qa_item) # inspect the item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823453c2-b9ef-490f-ace8-e411348da498",
   "metadata": {},
   "source": [
    "And the following function evaluates the performance of your `vanilla_qa` function on a list of qa_items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4a6c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa(qa_function, qa_items, verbose=False):\n",
    "    results = []\n",
    "\n",
    "    \n",
    "    for i, qa_item in tqdm.tqdm(enumerate(qa_items), desc=\"Evaluating QA instances\", total=len(qa_items)):\n",
    "\n",
    "        question = qa_item['question'] \n",
    "        answer = qa_item['answer']\n",
    "        context = qa_item['context']\n",
    "        \n",
    "        predicted_answer = qa_function(qa_item)\n",
    "\n",
    "        exact_match = compute_exact(answer, predicted_answer)\n",
    "        f1_score = compute_f1(answer, predicted_answer)\n",
    "        rouge2_f1 = compute_rouge2(answer, predicted_answer)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"Gold Answer: {answer}\")\n",
    "            print(f\"Predicted Answer: {answer}\")\n",
    "            print(f\"Exact Match: {exact_match}, F1 Score: {f1_score}\")\n",
    "            print(f\"ROUGE-2 F1 Score: {rouge2_f1}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"context\": context if context else None,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"rouge2_f1\": rouge2_f1\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc60afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [00:48<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "vanilla_evaluation_results = evaluate_qa(vanilla_qa, evaluation_benchmark['qas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869c4dc-5c55-4003-8bed-c68966fdbc98",
   "metadata": {},
   "source": [
    "The function returns a list of evaluation results, one dictionary for each qa item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9b4c4fe-51b2-40bb-b2c0-91dd572fbc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'predicted_answer': 'Christian Brothers of Ireland Stella Maris College',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'exact_match': 0,\n",
       " 'f1_score': 0.0,\n",
       " 'rouge2_f1': 0.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_evaluation_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658f34e-1b56-4c71-b4bb-54e88b6df3b6",
   "metadata": {},
   "source": [
    "Finally, the `present_results` function aggregates the results for the various qa items and prints the overall result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d72c6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_results(eval_results, exp_name=\"\"):\n",
    "    print(f\"{exp_name} Evaluation Results:\")\n",
    "    exact_matches = [res['exact_match'] for res in eval_results]\n",
    "    f1_scores = [res['f1_score'] for res in eval_results]\n",
    "    rouge2_f1 = [res['rouge2_f1'] for res in eval_results]\n",
    "    print(f\"Exact Match: {sum(exact_matches) / len(exact_matches) * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {sum(f1_scores) / len(f1_scores) * 100:.2f}%\")\n",
    "    print(f\"ROUGE2 F1: {sum(rouge2_f1) / len(rouge2_f1) * 100:.2f}%\")\n",
    "\n",
    "    # print out some evaluation results\n",
    "    for res in eval_results[:5]:\n",
    "        print(f\"Question: {res['question']}\")\n",
    "        print(f\"Gold Answer: {res['answer']}\")\n",
    "        print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
    "        print(f\"Exact Match: {res['exact_match']}, F1 Score: {res['f1_score']}\")\n",
    "        print(\"ROUGE-2 F1-score:\", res['rouge2_f1'])\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb2bf077-4292-4736-bf04-146f846daa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla QA Evaluation Results:\n",
      "Exact Match: 7.20%\n",
      "F1 Score: 13.85%\n",
      "ROUGE2 F1: 4.11%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Christian Brothers of Ireland Stella Maris College\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: 1:1\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1894\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: 1997\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: treating the mitrailleuse like a machine gun\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "present_results(vanilla_evaluation_results, \"Vanilla QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785b7b5-bb3b-49da-9699-e25c18a014df",
   "metadata": {},
   "source": [
    "**TODO:** Experiment with the prompt template and try to achieve an Exact Match score of at least 5%. You may want to try including an example in the prompt (single-shot prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2e479",
   "metadata": {},
   "source": [
    "## Part 3 - Oracle Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011bec1-e942-46d9-a193-4c8944e8b3f9",
   "metadata": {},
   "source": [
    "We will now establish an upper bound for a retrieval augmented QA system by providing the correct (\"gold\") context for each question as part of the prompt. These contexts are available as part of each qa_item in the evaluation_benchmark['qas'] dictionary. \n",
    "\n",
    "**TODO**: Write a function `oracle_qa(qa_item)` that takes in a qa_item, inserts both the question **and** the gold context into a prompt template, then passes the prompt to the LLM and returns the answer. The function should behave like the `vanilla_qa` function above, so that we can evaluate it using the same evaluation steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c69a364b-2cbf-4206-8bac-d806d7688a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_qa(qa_item): # Write this function\n",
    "    question = qa_item[\"question\"]\n",
    "    context = qa_item[\"context\"]\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a question answering system.\\n\"\n",
    "        \"Using ONLY the context below, extract the shortest exact text span that answers the question.\\n\"\n",
    "        \"Return ONLY the answer span exactly as it appears in the context. Do not add any extra words.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated = output[0][\"generated_text\"]\n",
    "    answer = generated[len(prompt):].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aeb37-dc13-4e30-be8d-eca28c6376fa",
   "metadata": {},
   "source": [
    "**TODO**: run the `evaluate_qa` function on your `oracle_qa` function and display the results. You should see Exact Match scores above 50% (if not, tinker with the prompt template). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cf95161-ebfc-4b99-a263-5ad8f230f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [01:49<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation Results:\n",
      "Exact Match: 53.60%\n",
      "F1 Score: 69.84%\n",
      "ROUGE2 F1: 38.07%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni\n",
      "Exact Match: 0, F1 Score: 0.8571428571428571\n",
      "ROUGE-2 F1-score: 0.8\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: about six to four\n",
      "Exact Match: 1, F1 Score: 1.0\n",
      "ROUGE-2 F1-score: 1.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1890s\n",
      "Exact Match: 1, F1 Score: 1.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: Government of Ireland Act 1914\n",
      "Exact Match: 0, F1 Score: 0.33333333333333337\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: French gunners with no experience\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oracle_evaluation_results = evaluate_qa(oracle_qa, evaluation_benchmark['qas'])\n",
    "present_results(oracle_evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee3d53",
   "metadata": {},
   "source": [
    "## Part 4 - Retrieval-Augmented Question Answering - Word Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae47c1-230c-4d39-9670-c82f18ac3f4c",
   "metadata": {},
   "source": [
    "Next, we will experiment with various approaches for retrieving relevant contexts from the set of available contexts. We first get the list of all 19035 available candidate contexts from the evaluation_benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26c0c460-2b20-4ea4-855c-aa8840893be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_contexts = evaluation_benchmark[\"contexts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "804e3f34-de25-47f0-a33f-3234739cacf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19035"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfb4e746-6909-45ee-b6a6-4c3bc31949c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_contexts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bf018-6240-436f-b3e5-eef3f0be16d9",
   "metadata": {},
   "source": [
    "### Token Overlap Retriever \n",
    "Let's first experiment with a simple retriever based on word overlap. Given a question, we measure how many of its tokens appear in each of the candidate contexts. We then retrieve the k contexts with the highest overlap. \n",
    "\n",
    "**TODO:** Write the function `retrieve_overlap(question, contexts, top_k)` that takes in the question (a string) and a list of contexts (each context is a string). It should calculate the word overlap between the question and *each* context, and return a list of the *top_k* contexts with the highest overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb453256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word overlap retriever -- write this function\n",
    "def retrieve_overlap(question, contexts, top_k=5):\n",
    "    question_tokens = set(get_tokens(question))\n",
    "    scores_list = []\n",
    "\n",
    "    for context in contexts:\n",
    "        context_tokens = set(get_tokens(context))\n",
    "        overlap = len(question_tokens & context_tokens)\n",
    "        scores_list.append((overlap, context))\n",
    "\n",
    "    scores_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_contexts = [context for i, context in scores_list[:top_k]]\n",
    "\n",
    "    return top_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733df5da-00ba-4507-a303-3a35d1753dd6",
   "metadata": {},
   "source": [
    "The following function runs the retriever a list of qa_items. For each qa_item it obtains the list of retrieved contexts and adds them to the qa_item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70979681-8b48-499e-9777-296863955856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rag_context(qa_items, contexts, retriever, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    for inst in tqdm.tqdm(result_items, desc=\"Retrieving contexts\"):\n",
    "        question = inst['question']\n",
    "        retrieved_contexts = retriever(question, contexts, top_k)\n",
    "        inst['rag_contexts'] = retrieved_contexts   \n",
    "    return result_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e2d07b0-9722-4241-b568-04b04ecd900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [03:34<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_qa_pairs = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, retrieve_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe4dd0-4849-4ec3-9e85-834d0a974e4a",
   "metadata": {},
   "source": [
    "It returns a copy of the qa_item list that is now annotated with the additional 'rag_contexts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a27e813c-c436-4bf8-80fd-e7df55b24024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       "  \"Red is one of the most common colors used on national flags. The use of red has similar connotations from country to country: the blood, sacrifice, and courage of those who defended their country; the sun and the hope and warmth it brings; and the sacrifice of Christ's blood (in some historically Christian nations) are a few examples. Red is the color of the flags of several countries that once belonged to the former British Empire. The British flag bears the colors red, white, and blue; it includes the cross of Saint George, patron saint of England, and the saltire of Saint Patrick, patron saint of Ireland, both of which are red on white. The flag of the United States bears the colors of Britain, the colors of the French tricolore include red as part of the old Paris coat of arms, and other countries' flags, such as those of Australia, New Zealand, and Fiji, carry a small inset of the British flag in memory of their ties to that country. Many former colonies of Spain, such as Mexico, Colombia, Ecuador, Cuba, Puerto Rico, Peru, and Venezuela, also feature red-one of the colors of the Spanish flag-on their own banners. Red flags are also used to symbolize storms, bad water conditions, and many other dangers. Navy flags are often red and yellow. Red is prominently featured in the flag of the United States Marine Corps.\",\n",
       "  'In July 2015, Eton accidentally sent emails to 400 prospective students, offering them conditional entrance to the school in September 2017. The email was intended for nine students, but an IT glitch caused the email to be sent to 400 additional families, who didn\\'t necessarily have a place. In response, the school issued the following statement: \"This error was discovered within minutes and each family was immediately contacted to notify them that it should be disregarded and to apologise. We take this type of incident very seriously indeed and so a thorough investigation, overseen by the headmaster Tony Little and led by the tutor for admissions, is being carried out to find out exactly what went wrong and ensure it cannot happen again. Eton College offers its sincere apologies to those boys concerned and their families. We deeply regret the confusion and upset this must have caused.\"',\n",
       "  'John Evans, for whom Evanston is named, bought 379 acres (153 ha) of land along Lake Michigan in 1853, and Philo Judson developed plans for what would become the city of Evanston, Illinois. The first building, Old College, opened on November 5, 1855. To raise funds for its construction, Northwestern sold $100 \"perpetual scholarships\" entitling the purchaser and his heirs to free tuition. Another building, University Hall, was built in 1869 of the same Joliet limestone as the Chicago Water Tower, also built in 1869, one of the few buildings in the heart of Chicago to survive the Great Chicago Fire of 1871. In 1873 the Evanston College for Ladies merged with Northwestern, and Frances Willard, who later gained fame as a suffragette and as one of the founders of the Woman\\'s Christian Temperance Union (WCTU), became the school\\'s first dean of women. Willard Residential College (1938) is named in her honor. Northwestern admitted its first women students in 1869, and the first woman was graduated in 1874.',\n",
       "  'Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family\\'s ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father\\'s tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_qa_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee220f-7e4d-4354-bfb0-32ef05b14492",
   "metadata": {},
   "source": [
    "Before we run an end-to-end evaluation, we can check the accuracy of the word overlap retriever. In other words, for what fraction of questions is the gold context included in the top-k retrieved contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b85399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric of retriever\n",
    "def evaluate_retriever(rag_qa_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the retriever by computing the accuracy of retrieved contexts against reference contexts.\n",
    "    \"\"\"\n",
    "    correct_retrievals = 0\n",
    "    for qa_item in rag_qa_pairs:\n",
    "        if qa_item['context'] in qa_item['rag_contexts']:\n",
    "            correct_retrievals += 1\n",
    "    accuracy = correct_retrievals / len(rag_qa_pairs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e6907-df91-41a7-813c-406aebba329f",
   "metadata": {},
   "source": [
    "In our implementation, we got an accuracy of 0.372. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b81b86eb-7789-4a17-87a6-c190955430e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retriever(rag_qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb3ff7-8d00-4e7b-8dbf-7b852ec1dfc8",
   "metadata": {},
   "source": [
    "**TODO**: Write a function `rag_qa(qa_item)` that behaves like the `vanilla_qa` and `oracle_qa` functions above. Create a prompt from the question and the top-k retrieved contexts (instead of the gold context you used in `oracle_qa`). You can assume that `qa_item` already \n",
    "contains the 'rag_contexts' field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4289b8bd-e21a-4248-bed9-efcd5d6a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(qa_item): # Write this function\n",
    "    question = qa_item[\"question\"]\n",
    "    rag_contexts = qa_item[\"rag_contexts\"]\n",
    "\n",
    "    context_block = \"\\n\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(rag_contexts)])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a question answering system.\\n\"\n",
    "        \"Using ONLY the contexts below, extract the shortest exact text span that answers the question.\\n\"\n",
    "        \"Return ONLY the answer span exactly as it appears in the contexts. Do not add any extra words.\\n\\n\"\n",
    "        f\"{context_block}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated = output[0][\"generated_text\"]\n",
    "    answer = generated[len(prompt):].strip()\n",
    "    # answer = answer.split(\"\\n\")[0].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d1181-df6b-4f18-b038-c83ad3d63ad7",
   "metadata": {},
   "source": [
    "**TODO**: Like you did for the vanilla and oracle qa system, evaluate the `rag_qa` function and display the results. In our implementation, we got an exact match of 19.6%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f85125b-a462-4355-92b9-c930e1e4cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [11:36<00:00,  2.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation Results:\n",
      "Exact Match: 31.20%\n",
      "F1 Score: 42.02%\n",
      "ROUGE2 F1: 21.98%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni\n",
      "Exact Match: 0, F1 Score: 0.8571428571428571\n",
      "ROUGE-2 F1-score: 0.8\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: 10.9%\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1890s\n",
      "Exact Match: 1, F1 Score: 1.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: 1908\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: a psycho-villain who is clearly out of his mind but seems to like it that way\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag_overlap_eval = evaluate_qa(rag_qa, rag_qa_pairs)\n",
    "present_results(rag_overlap_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca56df-f99d-4cf6-b877-cde203b102dc",
   "metadata": {},
   "source": [
    "## Part 5 - Retrieval-Augmented Question Answering - Dense Retrieval\n",
    "\n",
    "In this step, we will try to will encode each context and questions using BERT. We will then retrieve the k contexts whose embeddings have the highest cosine similarity to the question embedding.\n",
    "\n",
    "### 5.1 Creating Embeddings for Contexts and Questions \n",
    "\n",
    "Here is an example for how to use BERT to encode a sentence. Instead of using the CLS embeddings (as discussed in class) we will pool together the token representations at the last layer by averaging. The resulting representation is a (1,768) tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c3d6d9d-aaac-4708-8c40-ee8f06046275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "from transformers import BertTokenizer, BertModel # If you run into memory issues, you \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "inputs = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "with torch.no_grad(): \n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = torch.mean(hidden_states, dim=1)  # (batch_size=1, embedding size =768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18093c30-2133-4b0c-a5be-15534f6f373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7db1f-025e-4c70-a4c8-03d70bf433de",
   "metadata": {},
   "source": [
    "**TODO**: Write code to encode each candidate context. Stack the embeddings together into a single (19035, 768) pytorch tensor that we can save to disk and reload as needed (see above for how to access the candidate contexts). On some lower-resource systems you may have trouble instantiating both BERT and OLMo2 at the same time. Storing the encoded representations allows you to run just OLMo for the QA part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f7c617ff-8a22-451d-8144-b29d959e7bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding contexts: 100%|██████████| 19035/19035 [08:14<00:00, 38.47it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    # context_embeddings = ... \n",
    "    context_embeddings = []\n",
    "\n",
    "    for context in tqdm.tqdm(candidate_contexts, desc=\"Encoding contexts\"):\n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        embedding = torch.mean(hidden_states, dim=1)\n",
    "        context_embeddings.append(embedding.cpu())\n",
    "\n",
    "context_embeddings = torch.cat(context_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b19f021-e9fc-43fa-a54d-b49f268c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(context_embeddings, \"context_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc61f3-00c8-493a-812b-0eed87969197",
   "metadata": {},
   "source": [
    "**TODO**: Similarly encode each question and stack the embeddings together into a single (250, 768) pytorch tensor that we can save to disk and reload as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53f85eda-0b12-4934-bd57-1ffadbfd73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|██████████| 250/250 [00:04<00:00, 58.87it/s]\n"
     ]
    }
   ],
   "source": [
    "question_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # question_embeddings = ...\n",
    "    for qa_item in tqdm.tqdm(evaluation_benchmark[\"qas\"], desc=\"Encoding questions\"):\n",
    "        question = qa_item[\"question\"]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            question,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        embedding = torch.mean(hidden_states, dim=1)\n",
    "        question_embeddings.append(embedding.cpu())\n",
    "\n",
    "question_embeddings = torch.cat(question_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f1f0207-180d-467f-8a79-0e0e6a45f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(question_embeddings, \"question_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a95403-40b2-4bb7-b36e-7a8236399a46",
   "metadata": {},
   "source": [
    "### 5.2 Similarity Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65ae157a-c526-4cc7-9d6c-95cd291398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = torch.load(\"context_embeddings.pt\")\n",
    "question_embeddings = torch.load(\"question_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031d32f-4a0d-42e2-ae87-b397e7a27ab7",
   "metadata": {},
   "source": [
    "**TODO**: Write a function `retrieve_cosine(question_embedding, contexts, context_embeddings)` that takes in the embedding for a single question (a [1,768] tensor), a list of contexts (each is a string), and the context embedding tensor [19035,768].\n",
    "Note that the indices of the context list and the rows of the context_embeddings tensor line up. i.e. `context_embeddings[0]` is the embedding for `contexts[0]`, etc.\n",
    "You can use `torch.nn.functional.cosine_similarity` (or `F.cosine_similarity` since we imported `torch.nn.functional` as `F`, which is conventional) to calculate the similarities efficiently. You may also ant to look at `torch.topk`, but other solutions are possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40603f69-f627-4b68-9ef2-4019c78b5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cosine(question_emb, contexts, context_embeddings, top_k=5):\n",
    "        if question_emb.dim() == 2:\n",
    "            question_vec = question_emb.squeeze(0)\n",
    "        else:\n",
    "            question_vec = question_emb\n",
    "\n",
    "        sims = F.cosine_similarity(context_embeddings, question_vec.unsqueeze(0), dim=1)\n",
    "        vals, top_idx = torch.topk(sims, k=top_k)\n",
    "\n",
    "        return [contexts[i] for i in top_idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a56ef08f-39a3-4a5b-94a7-e70d139541e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
       " 'The University of St Mark & St John (known as \"Marjon\" or \"Marjons\") specialises in teacher training, and offers training across the country and abroad.',\n",
       " \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
       " \"Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013[update] there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.\"]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_cosine(question_embeddings[0], candidate_contexts, context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5a9c-b04c-421e-92f9-7b4fcabc0946",
   "metadata": {},
   "source": [
    "**TODO**: Write a new version of the add_rag_context function we provided above. This function should now additionally take the question embeddings and context embeddings as parameters, run the retrieval for each question (using the retrieve_cosine function above) and populate a new list of qa_items, include the selected 'rag_contexts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ea939c2-cc28-43b9-a1b2-547ce8042e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rag_context(qa_items, contexts, retriever, question_embeddings, context_embeddings, top_k=5):\n",
    "    result_list = copy.deepcopy(qa_items)\n",
    "\n",
    "    for i, idx in tqdm.tqdm(enumerate(result_list), desc=\"Retrieving contexts\", total=len(result_list)):\n",
    "        question_emb = question_embeddings[i].unsqueeze(0)\n",
    "        idx[\"rag_contexts\"] = retriever(question_emb, contexts, context_embeddings, top_k)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cb2a6add-eec1-40c1-a0b4-8368661ca6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts: 100%|██████████| 250/250 [00:02<00:00, 90.22it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_qa_items = add_rag_context(evaluation_benchmark['qas'], candidate_contexts, retrieve_cosine, question_embeddings, context_embeddings, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cd9708cb-961d-4aaa-9d6b-5c62895adf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
       " 'answer': 'professor Juan Pedro Toni',\n",
       " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
       "  'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
       "  'The University of St Mark & St John (known as \"Marjon\" or \"Marjons\") specialises in teacher training, and offers training across the country and abroad.',\n",
       "  \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
       "  \"Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013[update] there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.\"]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_qa_items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c0005-f4fd-4094-8e14-7f000bdc4c28",
   "metadata": {},
   "source": [
    "Run the `evaluate_retriever` function on the new qa_items. In our experiments, we got an accuracy of about 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f65ef204-69f9-47a4-8670-03aa6bea5277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.416"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retriever(rag_qa_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282ac18-789e-4f8b-9b78-6f64c671d736",
   "metadata": {},
   "source": [
    "Then, evaluate the rag_qa approach using the revised rag_qa_items. You should get an Exact match better than 20%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4cccddd0-731e-4807-82e8-b73ffa59838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA instances: 100%|██████████| 250/250 [06:11<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation Results:\n",
      "Exact Match: 18.00%\n",
      "F1 Score: 30.41%\n",
      "ROUGE2 F1: 15.41%\n",
      "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
      "Gold Answer: professor Juan Pedro Toni\n",
      "Predicted Answer: Juan Pedro Toni\n",
      "Exact Match: 0, F1 Score: 0.8571428571428571\n",
      "ROUGE-2 F1-score: 0.8\n",
      "----------------------------------------\n",
      "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
      "Gold Answer: about six to four\n",
      "Predicted Answer: 1:1\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
      "Gold Answer: 1890s\n",
      "Predicted Answer: 1896\n",
      "Exact Match: 0, F1 Score: 0.0\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: When did devolution in the UK begin?\n",
      "Gold Answer: 1914\n",
      "Predicted Answer: with the Government of Ireland Act 1914\n",
      "Exact Match: 0, F1 Score: 0.2857142857142857\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n",
      "Question: Treating the mitrailleuse like what rendered it far less effective\n",
      "Gold Answer: artillery\n",
      "Predicted Answer: like artillery and in this role it was ineffective\n",
      "Exact Match: 0, F1 Score: 0.19999999999999998\n",
      "ROUGE-2 F1-score: 0.0\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_qa(rag_qa, rag_qa_items)\n",
    "present_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373f0ec",
   "metadata": {},
   "source": [
    "## Part 6 - Experiments\n",
    "\n",
    "**TODO** For the overlap and dense retrievers (from part 5 and 6), what happens when you change the number of retrieved contexts? Present a table of results for k=1, k=5 (already done), k=10, and k=20. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810eb3",
   "metadata": {},
   "source": [
    "I tried to write some code for this part, to rerun my previous code with different k vakues, and then show them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2bced1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rag_context_overlap(qa_items, contexts, retriever, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    for item in tqdm.tqdm(result_items, desc=f\"Retrieving contexts (overlap, k={top_k})\"):\n",
    "        question = item[\"question\"]\n",
    "        retrieved_contexts = retriever(question, contexts, top_k)\n",
    "        item[\"rag_contexts\"] = retrieved_contexts\n",
    "    return result_items\n",
    "\n",
    "def add_rag_context_dense(qa_items, contexts, retriever, question_embeddings, context_embeddings, top_k=5):\n",
    "    result_items = copy.deepcopy(qa_items)\n",
    "    for i, item in tqdm.tqdm(enumerate(result_items), desc=f\"Retrieving contexts (dense, k={top_k})\", total=len(result_items)):\n",
    "        q_emb = question_embeddings[i].unsqueeze(0)\n",
    "        retrieved_contexts = retriever(q_emb, contexts, context_embeddings, top_k)\n",
    "        item[\"rag_contexts\"] = retrieved_contexts\n",
    "    return result_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a0b9f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_experiment_overlap(qa_items, candidate_contexts, ks=(1, 5, 10, 20), label=\"Overlap\"):\n",
    "    results = []\n",
    "    for k in ks:\n",
    "        rag_items = add_rag_context_overlap(qa_items, candidate_contexts, retrieve_overlap, top_k=k)\n",
    "        eval_results = evaluate_qa(rag_qa, rag_items)\n",
    "\n",
    "        exact = sum(r[\"exact_match\"] for r in eval_results) / len(eval_results) * 100\n",
    "        f1 = sum(r[\"f1_score\"] for r in eval_results) / len(eval_results) * 100\n",
    "        rouge = sum(r[\"rouge2_f1\"] for r in eval_results) / len(eval_results) * 100\n",
    "\n",
    "        results.append({\n",
    "            \"Retriever\": label,\n",
    "            \"k\": k,\n",
    "            \"Exact Match (%)\": round(exact, 2),\n",
    "            \"F1 (%)\": round(f1, 2),\n",
    "            \"ROUGE-2 (%)\": round(rouge, 2),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_rag_experiment_dense(qa_items, candidate_contexts, question_embeddings, context_embeddings, ks=(1, 5, 10, 20), label=\"Dense\"):\n",
    "    results = []\n",
    "    for k in ks:\n",
    "        rag_items = add_rag_context_dense(\n",
    "            qa_items, candidate_contexts, retrieve_cosine,\n",
    "            question_embeddings, context_embeddings,\n",
    "            top_k=k\n",
    "        )\n",
    "        eval_results = evaluate_qa(rag_qa, rag_items)\n",
    "\n",
    "        exact = sum(r[\"exact_match\"] for r in eval_results) / len(eval_results) * 100\n",
    "        f1 = sum(r[\"f1_score\"] for r in eval_results) / len(eval_results) * 100\n",
    "        rouge = sum(r[\"rouge2_f1\"] for r in eval_results) / len(eval_results) * 100\n",
    "\n",
    "        results.append({\n",
    "            \"Retriever\": label,\n",
    "            \"k\": k,\n",
    "            \"Exact Match (%)\": round(exact, 2),\n",
    "            \"F1 (%)\": round(f1, 2),\n",
    "            \"ROUGE-2 (%)\": round(rouge, 2),\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts (overlap, k=1): 100%|██████████| 250/250 [03:31<00:00,  1.18it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [02:15<00:00,  1.85it/s]\n",
      "Retrieving contexts (overlap, k=5): 100%|██████████| 250/250 [03:32<00:00,  1.18it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [09:02<00:00,  2.17s/it]\n",
      "Retrieving contexts (overlap, k=10): 100%|██████████| 250/250 [03:32<00:00,  1.18it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [18:54<00:00,  4.54s/it]\n",
      "Retrieving contexts (dense, k=1): 100%|██████████| 250/250 [00:03<00:00, 66.09it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [02:20<00:00,  1.78it/s]\n",
      "Retrieving contexts (dense, k=5): 100%|██████████| 250/250 [00:04<00:00, 57.26it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [05:52<00:00,  1.41s/it]\n",
      "Retrieving contexts (dense, k=10): 100%|██████████| 250/250 [00:04<00:00, 59.85it/s]\n",
      "Evaluating QA instances: 100%|██████████| 250/250 [11:47<00:00,  2.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>k</th>\n",
       "      <th>Exact Match (%)</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>ROUGE-2 (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overlap</td>\n",
       "      <td>1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>32.03</td>\n",
       "      <td>14.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Overlap</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2</td>\n",
       "      <td>42.02</td>\n",
       "      <td>21.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overlap</td>\n",
       "      <td>10</td>\n",
       "      <td>31.2</td>\n",
       "      <td>42.28</td>\n",
       "      <td>23.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dense</td>\n",
       "      <td>1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>18.23</td>\n",
       "      <td>8.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.41</td>\n",
       "      <td>15.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dense</td>\n",
       "      <td>10</td>\n",
       "      <td>23.2</td>\n",
       "      <td>35.56</td>\n",
       "      <td>18.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Retriever   k  Exact Match (%)  F1 (%)  ROUGE-2 (%)\n",
       "0   Overlap   1             23.2   32.03        14.63\n",
       "1   Overlap   5             31.2   42.02        21.98\n",
       "2   Overlap  10             31.2   42.28        23.02\n",
       "3     Dense   1             11.2   18.23         8.13\n",
       "4     Dense   5             18.0   30.41        15.41\n",
       "5     Dense  10             23.2   35.56        18.80"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ks = (1, 5, 10)\n",
    "\n",
    "overlap_results = run_rag_experiment_overlap(\n",
    "    evaluation_benchmark[\"qas\"],\n",
    "    candidate_contexts,\n",
    "    ks=ks,\n",
    "    label=\"Overlap\"\n",
    ")\n",
    "\n",
    "dense_results = run_rag_experiment_dense(\n",
    "    evaluation_benchmark[\"qas\"],\n",
    "    candidate_contexts,\n",
    "    question_embeddings,\n",
    "    context_embeddings,\n",
    "    ks=ks,\n",
    "    label=\"Dense\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(overlap_results + dense_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a61d22-65b5-4be0-b607-9afb9be97623",
   "metadata": {},
   "source": [
    "## Part 7 -Improving the QA System \n",
    "\n",
    "**TODO**\n",
    "In this part, we ask you to come up with one interesting or novel idea for improving the QA system. Your system does *not* have to outperform the models from part 4 or 5, but for full credit you should implement at least one new idea, beyond just changing parameters. You can either work on better retrieval or better QA/LLM performance. Show the full code for the necessary steps and evaluation results. \n",
    "\n",
    "Ideas for improving the retriever include: improved word overlap (better tokenization/ text normalization, using TF-IDF, ...), or choosing a different approach or different model (other than BERT) for calculating context and question embeddings.\n",
    "\n",
    "For the LLM, you could try a different transformer model, including text-to-text models (e.g. T5).                                                                                                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b5f4afb6-3f22-48cd-8fc9-51b0b9b44a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "context_tfidf = tfidf_vectorizer.fit_transform(candidate_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b15a7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tfidf(question, contexts, context_tfidf_matrix, vectorizer, top_k=5):\n",
    "    q_vec = vectorizer.transform([question])\n",
    "    scores = (context_tfidf_matrix @ q_vec.T).toarray().squeeze(1)\n",
    "\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return [contexts[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32dcca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contexts (TF-IDF): 100%|██████████| 250/250 [00:00<00:00, 677.01it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_qa_items_tfidf = copy.deepcopy(evaluation_benchmark[\"qas\"])\n",
    "\n",
    "for inst in tqdm.tqdm(rag_qa_items_tfidf, desc=\"Retrieving contexts (TF-IDF)\"):\n",
    "    inst[\"rag_contexts\"] = retrieve_tfidf(\n",
    "        inst[\"question\"],\n",
    "        candidate_contexts,\n",
    "        context_tfidf,\n",
    "        tfidf_vectorizer,\n",
    "        top_k=5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
